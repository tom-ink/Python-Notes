{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1> Statistical Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Numpy\n",
    "import numpy as np\n",
    "\n",
    "#? Creating array\n",
    "#* Creating 4x2 numpy array\n",
    "n1 = np.array([[1,2],[3,4],[5,6],[7,8]], dtype=np.int64)\n",
    "# dtype = np.int64, np.float64\n",
    "\n",
    "#* Creating 2x3 zero array\n",
    "n1 = np.zeros((2,3))\n",
    "\n",
    "#* Creating 1x4 one array\n",
    "n1 = np.ones((1,4))\n",
    "\n",
    "#* Creating 5x5 constant array\n",
    "n1 = np.full((5,5), 100)                    # Only takes integer\n",
    "\n",
    "#* Creating array of same element \n",
    "n1 = np.repeat([0.5, 0.25], 3, axis=0)      # Takes float too\n",
    "\n",
    "#* Creating 2x2 random array\n",
    "n1 = np.random.random((2,2))\n",
    "\n",
    "#* Creating 1xn increasing array\n",
    "n1 = np.arange(start=0, stop=11, step=1, dtype=int)\n",
    "np.arange('2020-04-01', '2024-07-15', 3, dtype='datetime64[M]').astype('datetime64[D]')                             # Create quarterly datetime\n",
    "np.arange(np.datetime64('2022-04-01'), np.datetime64('2023-05-01'), np.timedelta64(1, 'M'), dtype='datetime64[M]')  # Create monthly datetime\n",
    "\n",
    "#* Return evenly spaced array\n",
    "n1 = np.linspace(0, 10, num=10)\n",
    "\n",
    "#* Append values to the end of array\n",
    "n1 = np.append([1,2,3],[4,5,6], axis=1)\n",
    "\n",
    "\n",
    "#? Array shape\n",
    "#* Check shape of array\n",
    "n1.shape\n",
    "\n",
    "#* Reshape array\n",
    "n1.reshape((5,1))\n",
    "\n",
    "#* Return an array of ones/bool with same shape\n",
    "np.ones_like(n1)\n",
    "np.ones_like(n1, dtype=bool)    # Return bool instead of 1\n",
    "\n",
    "#* Return upper triangle of array\n",
    "np.triu(n1)\n",
    "\n",
    "\n",
    "#? Slicing array\n",
    "#* Return individual value\n",
    "x = n1[2,3]\n",
    "\n",
    "#* Return subarray\n",
    "n2 = n1[:2, 1:3]\n",
    "\n",
    "#* Replace value\n",
    "n1[0,0] = 100\n",
    "\n",
    "\n",
    "#? Math\n",
    "#* Add arrays together\n",
    "n3 = n1 + n2\n",
    "\n",
    "#* Sum up array\n",
    "n3 = np.sum(n1)\n",
    "\n",
    "#* Sum up array by column/row\n",
    "n3 = np.sum(n1, axis=0)\n",
    "# column: axis=0, row: axis=1\n",
    "\n",
    "#* Average array\n",
    "n3 = np.mean(n1)\n",
    "n3 = np.mean(n1, axis=0)\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Generate random variables\n",
    "import pandas as pd\n",
    "from scipy import stats\n",
    "import random\n",
    "\n",
    "random.seed(1234)       # for random\n",
    "np.random.seed(1234)    # for numpy and scipy\n",
    "\n",
    "\n",
    "#? Generate univariate random variables\n",
    "#* Normal variables\n",
    "random.normalvariate(mu=0, sigma=1)\n",
    "np.random.normal(loc=0, scale=1, size=100)\n",
    "stats.norm.rvs(loc=0, scale=1, size=100)\n",
    "\n",
    "#* t variables\n",
    "np.random.standard_t(10, size=100)\n",
    "stats.t.rvs(10, size=100)\n",
    "\n",
    "#* Uniform variables\n",
    "np.random.uniform(low=0, high=1, size=100)\n",
    "\n",
    "#* Discrete variables\n",
    "np.random.randint(low=10, high=100, size=(4,4))\n",
    "\n",
    "\n",
    "#? Generate multivariate random variables\n",
    "#* Normal variables\n",
    "np.random.multivariate_normal(mean=[0,0], cov=[[1,0], [0,100]], size=100)\n",
    "\n",
    "\n",
    "#? Random sampling\n",
    "np.random.choice(10,3,replace=True)     # from np.arange(10) choose 3 with replacement\n",
    "np.random.choice([.1,.3,.5,.7],3)       # choose from list without replacement\n",
    "\n",
    "\n",
    "#? Generate sampling distribution (mean)\n",
    "#* Simulated\n",
    "popu = np.random.normal(loc=65, scale=15, size=100000)  # start with known list of N population units\n",
    "samp = np.random.choice(popu, (25000,500))              # randomly select n units from list, multiple times\n",
    "samp_mean = samp.mean(axis=1)                           # calculate sample means\n",
    "# population size, N = 100000\n",
    "# sample size, n = 500\n",
    "# number of samples taken = 25000\n",
    "# every population unit has equal probability of selection = n/N\n",
    "# estimates of parameters based on SRS are UNBIASED (equal to population values on average)\n",
    "\n",
    "#* Subsampling\n",
    "df1 = pd.read_csv('probability_sampled_data.csv')\n",
    "for m in 100, 200, 400, 800:     # Subsample size\n",
    "    mean_diff = []  # Storage for our subsample mean differences\n",
    "    for i in range(1000):\n",
    "        dx = df1.sample(2*m)    # We need two subsamples of size m\n",
    "        dx1 = dx.iloc[0:m, :]   # First subsample\n",
    "        dx2 = dx.iloc[m:, :]    # Second subsample\n",
    "        mean_diff.append(dx1.BPXSY1.mean() - dx2.BPXSY1.mean())  # The difference of mean values\n",
    "# (sub)sample size, n = 100, 200, 400, 800\n",
    "# number of samples taken = 1000\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Inferential statistics\n",
    "import statsmodels.api as sm\n",
    "\n",
    "\n",
    "#? z-critical value\n",
    "stats.norm.ppf(0.05)            # left tail test\n",
    "stats.norm.ppf(1-0.05)          # right tail test\n",
    "stats.norm.ppf(1-0.05/2)          # two tail test\n",
    "# area leftwards of returned critical value\n",
    "\n",
    "#? t-critical value\n",
    "stats.t.ppf(q=0.05, df=99)          # left tail test\n",
    "stats.t.ppf(q=1-0.05, df=99)        # right tail test\n",
    "stats.t.ppf(q=1-0.025, df=99)         # two tail test\n",
    "# q = area leftwards of returned critical value\n",
    "\n",
    "\n",
    "\n",
    "#? p-value (z-statistic)\n",
    "stats.norm.cdf(-0.77)               # left tail test\n",
    "1-stats.norm.cdf(1.87)              # right tail test \n",
    "stats.norm.cdf(-1.24)*2             # two tail test\n",
    "\n",
    "stats.norm.sf(abs(-0.77))           # left tail test\n",
    "stats.norm.sf(abs(1.87))            # right tail test \n",
    "stats.norm.sf(abs(1.24))*2          # two tail test\n",
    "\n",
    "#? p-value (t-statistic)\n",
    "stats.t.cdf(-0.5,14)                # left tail test\n",
    "1-stats.t.cdf(1.5,14)               # right tail test \n",
    "stats.t.cdf(-0.5,14)*2              # two tail test\n",
    "\n",
    "stats.t.sf(abs(-0.5), df=14)        # left tail test\n",
    "stats.t.sf(abs(1.5), df=14)         # right tail test \n",
    "stats.t.sf(abs(0.5), df=14)*2       # two tail test\n",
    "\n",
    "\n",
    "\n",
    "# CI = Best Estimate +/- Margin of Error\n",
    "# Margin of Error = z*/t* x (estimated) standard error\n",
    "\n",
    "#? Confidence interval - Population Proportion\n",
    "# standard error for population proportion = sqrt(P_hat*(1-P_hat)/n)\n",
    "#* manually\n",
    "z_star = stats.norm.ppf(0.05/2)     # 95% CI, should be around 1.96 / 2\n",
    "p = .75\n",
    "n = 500\n",
    "se = np.sqrt((p*(1-p))/n)\n",
    "lcb = p - z_star*se\n",
    "ucb = p + z_star*se\n",
    "(lcb, ucb)\n",
    "\n",
    "#* statsmodels\n",
    "sm.stats.proportion_confint(n*p, n, alpha=0.05, method='normal')   # Confidence interval for a binomial population proportion\n",
    "# count: number of success; nobs: total number of trials; alpha: significance level\n",
    "\n",
    "\n",
    "\n",
    "#? Confidence interval - Difference in Population Proportions\n",
    "# standard error for difference in two population proportion = sqrt(se1^2 + se2^2 ) = sqrt(P_hat_1*(1-P_hat_1)/n1 + P_hat_2*(1-P_hat_2)/n2)\n",
    "#* manually\n",
    "z_star = stats.norm.ppf(0.05/2)\n",
    "p1 = .304845\n",
    "n1 = 3000\n",
    "se_1 = np.sqrt(p1 * (1 - p1)/n1)\n",
    "\n",
    "p2 = .513258\n",
    "n2 = 2750\n",
    "se_2 = np.sqrt(p2 * (1 - p2)/n2)\n",
    "\n",
    "se_diff = np.sqrt(se_1**2 + se_2**2)\n",
    "\n",
    "d = p1 - p2\n",
    "lcb = d - z_star * se_diff\n",
    "ucb = d + z_star * se_diff\n",
    "(lcb, ucb)\n",
    "\n",
    "\n",
    "\n",
    "#? Confidence interval - Mean\n",
    "# standard error for mean = SD/sqrt(n)\n",
    "#* manually\n",
    "t_star = stats.t.ppf(0.05/2, 999)   # 95% CI, df=n-1\n",
    "mean = df1['column1'].mean()\n",
    "sd = df1['column1'].std(ddof=1)     # ddof=1 to find sample SD\n",
    "n = len(df1)\n",
    "se = sd/np.sqrt(n)      # can also calculate sem using scipy.stats.sem()\n",
    "lcb = mean - t_star*se\n",
    "ucb = mean + t_star*se\n",
    "(lcb, ucb)\n",
    "\n",
    "#* scipy\n",
    "stats.norm.interval(0.95, loc=mean, scale=sd/np.sqrt(n))\n",
    "stats.t.interval(0.95, loc=mean, scale=sd/np.sqrt(n))\n",
    "\n",
    "#* statsmodels\n",
    "sm.stats.DescrStatsW(df1['column1']).zconfint_mean(alpha=0.05, alternative='two-sided')\n",
    "sm.stats.DescrStatsW(df1['column1']).tconfint_mean(alpha=0.05, alternative='two-sided')    # two-sided confidence interval for weighted mean of data\n",
    "# For alternative, 'two-sided'-> H1: mean != value, 'larger'-> H1: mean > value, 'smaller'-> H1: mean < value\n",
    "\n",
    "\n",
    "\n",
    "#? Confidence interval - Difference in Independent Population Means (unequal variance)\n",
    "# standard error for difference in two population mean = sqrt(se1^2 + se2^2 ) = sqrt(SD1^2/n1+SD2^2/n2)\n",
    "#* manually\n",
    "t_star = stats.t.ppf(0.05/2, 498) # df=n1+n2-2\n",
    "sem_1 = 7.753319 / np.sqrt(3000)            # can also calculate sem using scipy.stats.sem(), \n",
    "sem_2 = 6.252568 / np.sqrt(2750)            # by default, np.std uses ddof=0 while stats.sem uses ddof=1\n",
    "sem_diff = np.sqrt(sem_1**2 + sem_2**2)     # set ddof to 1 for sample SD, ddof to 0 for population SD\n",
    "\n",
    "d = 29.939946 - 28.778072   # mean_1 - mean_2\n",
    "lcb = d - t_star * sem_diff\n",
    "ucb = d + t_star * sem_diff\n",
    "(lcb, ucb)\n",
    "\n",
    "\n",
    "\n",
    "#? Confidence interval - Difference in Paired Population Means\n",
    "# standard error for differences in mean = SDd/sqrt(n)\n",
    "#* manually\n",
    "t_star = stats.t.ppf(0.05/2, 999)   # df=n-1\n",
    "diff = df1['column1'] - df1['column2']\n",
    "mean_d = diff.mean()\n",
    "sd_d = diff.std(ddof=1)\n",
    "n_d = len(diff)\n",
    "sem_d = sd_d/np.sqrt(n_d)\n",
    "lcb = mean_d - t_star * sem_d\n",
    "ucb = mean_d + t_star * sem_d\n",
    "(lcb, ucb)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# test statistics = (best estimate - hypothesised estimate) / standard error of estiamte\n",
    "\n",
    "#? Hypothesis testing - Population Proportion\n",
    "# z-statistic = (P_hat-P_0)/sqrt(P_0*(1-P_0)/n)\n",
    "z_stat, p_value = sm.stats.proportions_ztest(100, 250, value=0.5, alternative='two-sided', prop_var=0.5)\n",
    "# count: # success, nobs: # trials, value: hypothesised proportion / P_0\n",
    "# alternative: ‘two-sided’ (default), ‘less’, ‘greater’; choose based on H1\n",
    "# prop_var: the P to calculate standard error, should set to P_0 (by default prop_var = count/nobs aka P_hat)\n",
    "\n",
    "\n",
    "\n",
    "#? Hypothesis testing - Difference In Population Proportions\n",
    "# z-statistic = (P_hat1-P_hat2)-n)/sqrt(P_hat*(1-P_hat)*(1/n1+1/n2)), where P_hat=(#success1+#success2)/(n1+n2)\n",
    "#* statsmodels\n",
    "z_stat, p_value = sm.stats.proportions_ztest([100,375], [250,500], value=0, alternative='two-sided')\n",
    "# count: [#success1,#success2], nobs: [n1,n2]\n",
    "# value: hypothesised proportion = 0, # alternative: ‘two-sided’ (default), ‘less’, ‘greater’; choose based on H1\n",
    "\n",
    "\n",
    "\n",
    "#? Hypothesis testing - Population Mean\n",
    "# t-statistic = (X_bar-mu)/(SD/sqrt(n))\n",
    "#* t-test\n",
    "t_stat, p_value = stats.ttest_1samp(a=df1['column1'], popmean=100, alternative='two-sided')\n",
    "# popmean: hypothesised mean\n",
    "# alternative: ‘two-sided’ (default), ‘less’, ‘greater’\n",
    "\n",
    "#* z-test\n",
    "z_stat, p_value = sm.stats.ztest(df1['column1'], value=100, alternative='two-sided')\n",
    "# alternative: ‘two-sided’ (default), ‘larger’, ‘smaller’; choose based on H1\n",
    "\n",
    "\n",
    "\n",
    "#? Hypothesis testing - Difference in Independent Population Means\n",
    "# t-statistic (same variance) = ((X_bar_1-X_bar_2)-0)/(sqrt(((n1-1)*SD1^2+(n2-1)*SD2^2)/(n1+n2-2))*sqrt(1/n1+1/n2))\n",
    "# t-statistic (different variance) = ((X_bar_1-X_bar_2)-0)/sqrt(SD_1^2/n1+SD_2^2/n2)\n",
    "#* t-test\n",
    "t_stat, p_value = stats.ttest_ind(a=df1['column1'], b=df1['column2'], equal_var=True, alternative='two-sided')\n",
    "# H0: mu_a - mu_b = 0\n",
    "\n",
    "#* z-test\n",
    "z_stat, p_value = sm.stats.ztest(df1['column1'], df1['column2'], value=0, alternative='two-sided')    # same variance\n",
    "var_1 = sm.stats.DescrStatsW(df1['column1'])\n",
    "var_2 = sm.stats.DescrStatsW(df1['column2'])\n",
    "z_stat, p_value = sm.stats.CompareMeans(var_1, var_2).ztest_ind(usevar='pooled', alternative='two-sided', value=0)      # same variance\n",
    "z_stat, p_value = sm.stats.CompareMeans(var_1, var_2).ztest_ind(usevar='unequal', alternative='two-sided', value=0)     # different variance\n",
    "# H0: x1 - x2 = 0\n",
    "# alternative: ‘two-sided’ (default), ‘larger’, ‘smaller’; choose based on H1\n",
    "\n",
    "\n",
    "\n",
    "#? Hypothesis testing - Difference in Paired Population Means\n",
    "# t-statistic = (X_bar_d-0)/(SD_d/sqrt(n))\n",
    "#* t-test\n",
    "t_stat, p_value = stats.ttest_rel(a=df1['column1'], b=df1['column2'], alternative='two-sided')\n",
    "# H0: mu(a-b) = 0\n",
    "\n",
    "#* z-test\n",
    "z_stat, p_value = sm.stats.ztest(df1['column1']-df1['column2'], value=0, alternative='two-sided')\n",
    "# H0: mu_d = 0\n",
    "# alternative: ‘two-sided’ (default), ‘larger’, ‘smaller’; choose based on H1\n",
    "\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Descriptive Statistics\n",
    "<ul>\n",
    "    <li>Measures of central tendency:</li>\n",
    "    <ul>\n",
    "        <li>Mean</li>\n",
    "        <ul>\n",
    "            <li>Average value</li>\n",
    "            <li>For discrete (whole numbers) and continuous data (decimals)</li>\n",
    "            <li>Mean is susceptible to outliers and skewness. If the mean comes with large spread value, mean may not be representative.</li>\n",
    "            <li>Not for categorical data</li>\n",
    "        </ul>\n",
    "        <li>Median</li>\n",
    "        <ul>\n",
    "            <li>Middle value</li>\n",
    "            <li>Divides the distribution into half. Half of the data points are less than the median and the other half of them are more than the median.</li>\n",
    "            <li>Less susceptible to outliers and skewness</li>\n",
    "            <li>Not for categorical data</li>\n",
    "        </ul>\n",
    "        <li>Mode</li>\n",
    "        <ul>\n",
    "            <li>Most frequently occurring value</li>\n",
    "            <li>Suitable for discrete, continuous, and categorical data</li>\n",
    "            <li>In some data, mode may not be a good representation of centrality</li>\n",
    "        </ul>\n",
    "    </ul>\n",
    "    <li>Measures of Spread/Dispersion:</li>\n",
    "    <ul>\n",
    "        <li>Range</li>\n",
    "        <ul>\n",
    "            <li>Difference between the highest and lowest value in data</li>\n",
    "        </ul>\n",
    "        <li>Quartiles</li>\n",
    "        <ul>\n",
    "            <li>Divide data into quarters, four equal parts (Q1, Q2, and Q3) with Q2 sitting at the median (2nd quartile is the median)</li>\n",
    "        </ul>\n",
    "        <li>Variance</li>\n",
    "        <ul>\n",
    "            <li>Measures the width of its spread from center</li>\n",
    "            <li>Average squared difference between a variable’s value and the mean</li>\n",
    "            <li>Less variation/risk is preferred</li>\n",
    "        </ul>\n",
    "        <li>Standard Deviation</li>\n",
    "        <ul>\n",
    "            <li>Square root of variance</li>\n",
    "        </ul>\n",
    "    </ul>\n",
    "    <li>More important task is to make sense out of the data:</li>\n",
    "    <ul>\n",
    "        <li>What does the column's mean and standard deviation tells you?</li>\n",
    "        <li>Why is the mean and standard deviation so close to each other? What could be the reason for this?</li>\n",
    "    </ul>\n",
    "    <li>Pandas Codes:</li>\n",
    "    <ul>\n",
    "        <li>df.describe( )</li>\n",
    "        <li>df.mean( )</li>\n",
    "        <li>df.std( )</li>\n",
    "        <li>df.var( )</li>\n",
    "        <li>df['A'].cov(df['A'])</li>\n",
    "        <li>df.median( )</li>\n",
    "        <li>df.mode( )</li>\n",
    "        <li>df.quantile(q=0.5)</li>\n",
    "        <li>df.count( )</li>\n",
    "        <li>df.sum( )</li>\n",
    "        <li>df.min( )</li>\n",
    "        <li>df.max( )</li>\n",
    "        <li>df['A'].abs( )</li>\n",
    "        <li>df['A'].corr(method='pearson')</li>\n",
    "        <li>df.kurt( )</li>\n",
    "        <li>df.skew( )</li>\n",
    "    </ul>\n",
    "    <li>Numpy/Scipy Codes:</li>\n",
    "    <ul>\n",
    "        <li>stats.describe(n1)</li>\n",
    "        <li>np.mean(n1) <i>or</i> n1.mean( ) </li>\n",
    "        <li>np.var(n1) <i>or</i> n1.var()</li>\n",
    "        <li>np.min(n1) <i>or</i> n1.min( )</li>\n",
    "        <li>np.max(n1) <i>or</i> n1.max( )</li>\n",
    "    </ul>\n",
    "    <li>Statsmodels Codes:</li>\n",
    "    <ul>\n",
    "        <li>import statsmodels.api as sm</li>\n",
    "        <li>a = sm.stats.DescrStatsW(df['A'])</li>\n",
    "        <li>a.mean</li>\n",
    "        <li>a.std</li>\n",
    "        <li>a.var</li>\n",
    "    </ul>\n",
    "</ul>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Predictive Statistics\n",
    "import statsmodels.formula.api as smf\n",
    "\n",
    "#? Linear Regression\n",
    "#* Set up regression model & evaluate it\n",
    "m = smf.ols(\"y ~ X1 + C(X2, Treatment('Reference')) + X3*X4\", data=df1).fit()\n",
    "m2 = smf.ols(\"np.log(Y) ~ np.log(X)\", data=df1).fit()\n",
    "# Wrap categorical variables with C function; Use Treatment to specify reference group\n",
    "# Infer interaction effects using *\n",
    "\n",
    "#* Summary table\n",
    "m.summary()\n",
    "\n",
    "#* Parameter estimates\n",
    "m.params\n",
    "# intercept and coefficients\n",
    "\n",
    "#* Standard error of regression\n",
    "np.sqrt(m.mse_resid)\n",
    "\n",
    "#* RSQ value\n",
    "m.rsquared\n",
    "\n",
    "\n",
    "#? Logistic Regression (generalized linear model)\n",
    "#* Set up regression model & evaluate it\n",
    "m = smf.glm(\"y ~ X1 + X2\", data=df1, family=sm.families.Binomial()).fit()\n",
    "\n",
    "\n",
    "#? Marginal Linear Regression (using GEE to fit GLM to estimate intraclass correlation)\n",
    "#? when observations are possibly correlated within a cluster but uncorrelated across clusters\n",
    "#* Set up regression model & evaluate it\n",
    "m = smf.gee(\"y ~ 1\", data=df1, groups='column0', cov_struct=sm.cov_struct.Exchangeable()).fit()\n",
    "m.cov_struct.summary()\n",
    "\n",
    "\n",
    "#? Multilevel Models (when there's high potential for outcomes to be grouped together)\n",
    "for i in ['column1', 'column2', 'column3']:\n",
    "    m = smf.gee(i + ' ~ 1', groups='column0', cov_struct=sm.cov_struct.Exchangeable(), data=df1).fit()\n",
    "    print(i, m.cov_struct.summary())\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
