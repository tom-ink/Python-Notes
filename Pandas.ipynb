{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Creating Dataframe\n",
    "import pandas as pd\n",
    "\n",
    "#* Standard way of constructing DataFrame \n",
    "df1 = pd.DataFrame({'Yes': [50, 21], 'No': [131, 2]}, index=['Row A', 'Row B'])     #* Index = Row Labels\n",
    "\n",
    "#* Creating Series (a single column DataFrame)\n",
    "s1 = pd.Series([1,2,3], index=['Row A', 'Row B', 'Row C'], name='Has no Column Name, Only has one Overall Name')\n",
    "\n",
    "#* Assigning (new) data to Dataframe\n",
    "df1['column_name'] = 'fill all row with this value'\n",
    "df1['fill_with_iterable_values'] = range(len(df1))\n",
    "\n",
    "#* Inserting new data to Dataframe at specific position\n",
    "df1.insert(0, 'column_name', 'fill all row with this value')\n",
    "\n",
    "#* Arithmetic operators\n",
    "# Index (and column) alignment must match\n",
    "# Any item for which one or the other does not have an entry is marked with NaN\n",
    "df2 = df1 + df1\n",
    "# Use pandas arithmetic methods to override NaN (index & column alignment still exists)\n",
    "df1.add(df1, fill_value=0)\n",
    "\n",
    "#* Convert Series to DataFrame\n",
    "s1.to_frame()\n",
    "\n",
    "#* Convert Series to list\n",
    "s1.tolist()\n",
    "\n",
    "#* Import CSV file as DataFrame\n",
    "df1 = pd.read_csv('file.csv', keep_default_na=False)\n",
    "\n",
    "#* Export DataFrame into CSV file\n",
    "df1.to_csv('new_file_name.csv', mode='a', header=False, index=False)        \n",
    "# mode='a' -> append; add dataframes to existing csv. mode='w' -> write (default).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## iloc & loc\n",
    "#* Row-first, Column-second. (Native python does column-first, row-second)\n",
    "\n",
    "#* Index-based selection (iloc)\n",
    "df1.iloc[:-5, [0,1,2]]\n",
    "\n",
    "#* Label-based selection (loc)\n",
    "df1.loc[0:9,['Country', 'Name', 'Age']]\n",
    "\n",
    "#* For iloc, 0:10 -> 0,...,9    (return 10 entries)\n",
    "#* For loc, 0:10 -> 0,...,10    (return 11 entries)\n",
    "\n",
    "\n",
    "#* Selecting multiple specific columns\n",
    "import numpy as np\n",
    "df1.loc[:,np.isin(df1.columns,[col for col in df1.columns if \"ABC\" in col])]\n",
    "\n",
    "\n",
    "#* To create additional labels in index/column names (when you come up with a new, better index for the dataset)\n",
    "df1.set_index('new columns')\n",
    "\n",
    "\n",
    "# Use df1['column a'][0] when you know exactly what column name and/or row index you want\n",
    "# Use iloc and loc when you want to get a range of columns/rows (more than 1 column/row)\n",
    "# Use iloc if you want to slice columns via index numbers\n",
    "# NOT RECOMMENDED: df1['column'][rows]='values'. RECOMMENDED: df1.loc['rows','column']='values'."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### To understand the data\n",
    "| Operator          | Example                                                            |\n",
    "|-------------------|--------------------------------------------------------------------|\n",
    "| ``head()``        | ``df1.head()``                                                     |\n",
    "| ``shape``         | ``df1.shape``                                                      |\n",
    "| ``describe()``    | ``df1['Country'].describe()``                                      |\n",
    "| ``dtype``         | ``df1['Country'].dtype``                                           |\n",
    "| ``unique()``      | ``df1['Country'].unique()``                                        |\n",
    "| ``value_counts()``| ``df1['Country'].value_counts()``                                  |\n",
    "\n",
    "\n",
    "#### Useful Pandas built-in conditional selectors\n",
    "| Operator       | Example                                                            | Description    |\n",
    "|----------------|--------------------------------------------------------------------|----------------|\n",
    "| ``isin()``     | ``df1['Country'].isin(['Italy','France'])`` <br> ``df1.loc[~df1['Country'].isin(['Italy','France']),:]`` | Return ``True`` when value == ['xxx','yyy']. Useful for selecting rows with/without a particular (or multiple) value |\n",
    "| ``isnull()``   | ``df1['Country'].isnull()``                                        | Return ``True`` when value == NaN |\n",
    "| ``notnull()``  | ``df1['Country'].notnull()``                                       | Return ``True`` when value != NaN |\n",
    "\n",
    "\n",
    "#### Useful functions for data\n",
    "| Operator       | Example                                                            | Description                                                     |\n",
    "|----------------|--------------------------------------------------------------------|-----------------------------------------------------------------|\n",
    "| ``astype()``   | ``df1['Country'].astype(dtype, errors='ignore')``                  | dtype= ``int``, ``float``, ``str``, ``complex``, ``category``.|\n",
    "| ``fillna()``   | ``df1['Country'].fillna(value, method=None, inplace=False)``     | Replace all ``NaN`` with an explicit ``value``, or via method=``'bfill'``/``'backfill'``, ``'ffill'``/``'pad'``. <br> axis= ``'index'``, ``'columns'``. limit= ``None``, ``int``|\n",
    "| ``replace()``  | ``df1.replace(to_replace=['value1','value2'], value='value3')`` <br> ``df1.replace({value1:'new_value1', value2:'new_value2'})``    | Replace ``value`` in DataFrame. Can replace ``np.nan`` too.|\n",
    "| ``where()``  | ``df1.where(df % 3 == 0, 0, inplace=False)`` | Replace values where the condition is False. |\n",
    "| ``rename()``   | ``df1.rename(columns={'col1':'AA'})`` <br> ``df1.rename(index={0:'row_a'})`` | Change column names and/or index names. Use ``set_axis()`` to reassign entire column header. More convenient to use ``set_index()`` to rename index values |\n",
    "| ``rename_axis()``   | ``df1.rename_axis('new_name', axis='rows')``                  | axis= ``'rows'``, ``'columns'``. |\n",
    "| ``compare()``   | ``df1.compare(df2, keep_shape=False, keep_equal=False)``          | Compare one DataFrame to another, and show the differences. <br> keep_shape: keep all original rows & columns; keep_equal: keep all original values even if they are equal. |\n",
    "| ``concat()``   | ``pd.concat([df, df2, df3], axis=0)``                              | Combine different DataFrames/Series with the same fields (columns). <br> axis=``'index'``, ``'columns'``.  ignore_index=``False``, ``True``. |\n",
    "| ``merge()`` <br> (inner join) <br><br> [how=``inner``, ``outer``, ``left``, ``right``]  | ``pd.merge(df_left, df_right)`` <br><br> ``pd.merge(df_left, df_right, left_on='col1', right_on='colA')`` <br><br> ``pd.merge(df_left, df_right, left_index=True, right_on='colA')`` <br><br> ``pd.merge(df_left, df_right, left_index=True, right_index=True)``                    | To combine DataFrames with at least 1 common column. <br><br> If DataFrames don't have common columns, use left_on=``'join on this df_left column'``, right_on=``'join on this df_right column'`` <br><br> To join on one index (df_left has index set) <br><br> To join on two indexes (both df have indexes set)\n",
    "| ``join()``     | ``df_left.join(df_right)`` | Recommended To Use ``merge()`` instead.|\n",
    "| ``insert()``   | ``df.insert(1, 'col_name', s1)`` | Insert column into DataFrame at specified location. <br> ``loc``: insertion index. ``column``: column label. ``value``: Series or array-like. ``allow_duplicates=False``.|\n",
    "| ``drop()``     | ``df.drop(label, axis='index', index=None, columns=None, inplace=False)``| Drop specified rows/columns using ``label`` & ``axis``, or using ``index`` and/or ``columns``|\n",
    "| ``nlargest()`` <br><br> ``nsmallest()``   | ``df.nlargest(3, 'column1', keep='first')`` <br><br> ``df.nlargest(3, ['column1', 'column2'])`` <br><br> ``df.nsmallest(3, 'column1')`` | Return the first/last n rows ordered by columns in descending order. Similar to `df.sort_values(columns, ascending=False).head(n)` <br><br> `keep` = `first`: prioritize the first occurrence, `last`: prioritize the last  occurrence, `all`: do not drop any duplicates (may select more than n items) |\n",
    "| ``rank()``     | ``df.rank(method='average', numeric_only=False, na_option='keep')``| Compute numerical ranks (1 to n) along axis. <br> ``axis``: ``'index'``, ``'columns'``. ``method``: rank records with same values using ``'average'``, ``'min'``, ``'max'``, ``'first'``, ``'dense'``. ``na_option``: rank NaN values using ``keep``, ``top``, ``bottom``. ``ascending=True``: rank in ascending order. ``pct=False``: to display rankings in percentile. |\n",
    "| ``cut()``      | ``pd.cut(df['col1'], bin=3)``<br>``pd.cut(df['col1'], bins=[], labels=[], retbins=False)``| Bin values into discrete (categorical) intervals. <br> ``bins``: criteria to create bin [fixed width or between ranges]. ``labels``: add labels to bins. ``retbins``: return the bins. ``right=True``: inclusive of right limit. ``include_lowest=True``: include lowest bin. |\n",
    "| ``qcut()``     | ``pd.qcut(df['col1'], 10, labels=None, retbins=False)``| Create n number of equal-sized bins |\n",
    "| ``idxmin()``   | ``df1.idxmin()``| Return index of first occurrence of min value, specify across row/column using axis= ``0``, ``1`` |\n",
    "| ``idxmax()``   | ``df1.idxmax()``| Return index of first occurrence of max value, specify across row/column using axis= ``0``, ``1`` |\n",
    "| ``sample()``   | ``df1.sample(n=100, replace=False)``| Return a random sample of items. <br> ``n``: number of items to return. ``frac``: fraction of total items to return. ``replace``: replace sampled value. ``random_state``: seed. ``weight``: Default ``None`` result in equal probability weighting while larger weight means more likely to be sampled. |\n",
    "\n",
    "\n",
    "#### Grouping & Setting Index [Split-Apply-Combine]\n",
    "| Operator                  | Example                                                            | Description                           |\n",
    "|---------------------------|--------------------------------------------------------------------|---------------------------------------|\n",
    "| ``groupby()``             | ``df1.groupby('column1', as_index=True, group_keys=True).count()`` <br><br> ``df1.groupby(level=0, axis=0).sum()`` | aka ``value_counts()`` <br><br> Group by multiIndex, ``level``: index level, ``axis``: index/columns <br><br> `as_index`=`True`: return group labels as (multi)index <br> `group_keys`=`True`: set group keys as (multi)index |\n",
    "| ``groupby()``+``groups()`` | ``df1.groupby('column1').groups()`` <br><br> ``df1.groupby('column1').groups('value_A')`` | List all group name & group row indexes <br><br> List all row indexes of the group |\n",
    "| ``groupby()``+``get_group()`` | ``df1.groupby('column1').get_group('value_A')`` | Filtering by group |\n",
    "| ``groupby()``+``nth()`` | ``df1.groupby('column1').nth('1', dropna=None)`` <br><br> ``df1.groupby('column1').nth('-1')`` <br><br> ``df1.groupby('column1').nth([0,2,4])`` <br><br> ``df1.groupby('column1').nth(0:5)`` | Take the nth row from each group. <br><br> `dropna`: apply dropna before counting which row is the nth row, use `none` to not apply dropna, use `any` to apply dropna. <br><br> `df.groupby().nth[:N]` is same as `df.groupby().head(N)` |\n",
    "| ``groupby()``+``nlargest()`` <br><br> ``groupby()``+``nsmallest()`` | ``df1.groupby('column1')['column2'].nlargest()`` <br><br> ``df1.groupby('column1','column2)['column3'].sum().groupby('column1', group_keys=False).nsmallest()`` | Return largest n elements from each group <br><br> Return smallest n elements from each outer group (ignoring inner group) |\n",
    "| ``groupby()``+``apply()`` | ``df1.groupby('column1').apply(lambda df: df.column1.iloc[0])``    | Applying ``function`` to each group   |\n",
    "| ``groupby([])``+``apply()`` | ``df1.groupby(['column1','column2']).apply(lambda df: df.loc[df.column3.idxmax()])`` | Finding max ``col3's`` value for all ``col1``+``col2`` groups |\n",
    "| ``groupby()``+``agg()``   | ``df1.groupby('column1').agg(['mean', 'std', 'count'])`` <br><br> ``df1.groupby('column1').column2.agg([np.mean, lambda x:np.std(x, ddof=1), np.size])`` <br><br> ``df1.groupby('column1').agg({['column2': [min, max], 'column3': len})`` | Run many functions on all columns <br><br> Select a column for aggregation <br><br> Different aggregations per column |\n",
    "| ``for group_name, rows in df.groupby()`` | ``for group_name, rows in df1.groupby('column1'):``<br> &ensp; ``df2 = rows.loc[rows.column2==1, \"column3\"]`` | Loop through each groups; useful when creating/working with multiple objects |\n",
    "|                           |                                                                    |                                       |\n",
    "| ``pivot`` | ``df1.pivot(index='col1', columns='col2', values=['col3', 'col4'])`` | Less powerful version of pivot_table. <br><br> Index and columns selected cannot contain duplicate entries|\n",
    "| ``pivot_table`` | ``pd.pivot_table(df1, values='col4', index=['col1', 'col2'], columns=['col3', 'col4'], aggfunc=[np.sum, np.mean], fill_value=0`` <br><br> ``pd.pivot_table(df1, values='col4', index=['col1', 'col2'], aggfunc={'col3': np.mean, 'col4': [min, max, np.mean]}`` | Create spreadsheet-style pivot table. <br><br> Doesn't work when there are duplicate rows |\n",
    "|                           |                                                                    |                                       |\n",
    "| ``set_index()``           | ``df1.set_index(['columns1','columns2'])``                         | Set existing column as index          |\n",
    "| ``stack()``               | ``df1.stack(level=-1)``                                            | Convert column to index               |\n",
    "| ``unstack()``             | ``df1.unstack(level=-1)``                                          | Convert index to column               |\n",
    "| ``reset_index()``         | ``df1.reset_index(level=[0])``                                     | Convert multi-index back to regular index|\n",
    "| ``droplevel()``           | ``df1.droplevel(level='column_index', axis=0)``                    | Remove indexs/column indexs, use ``level=[]`` to specify level to drop |\n",
    "| ``melt()``                | ``pd.melt(df1, id_vars['col1'], value_vars=['col1'])``             | Unpivot DataFrame from wide to long   |\n",
    "\n",
    "\n",
    "#### Multi-index\n",
    "| Operator                  | Example                                                            | Description                           |\n",
    "|---------------------------|--------------------------------------------------------------------|---------------------------------------|\n",
    "| Getting a row             | ``df1.loc[('outer_index','inner_index'), 'column1']`` <br><br> ``df1.loc[('outer_index', ['inner_index_1', 'inner_index_2']),'column1']`` <br><br> ``df1.loc[('outer_index_1','inner_index_1'):('outer_index_2','inner_index_2')]`` <br><br> ``df1.loc[(slice(\"index_1\", \"index_2\"), slice(None), [\"index_C\", \"index_D\"]), :]`` <br><br> ``df1.loc[pd.IndexSlice[:, :, [\"index_C\", \"index_D\"]], :]`` | Indexing with MultiIndex  <br><br> slice(None) == \"slice nothing\" <br><br> UnsortedIndexError: 'MultiIndex slicing requires the index to be lexsorted <br> => need sort MultiIndex before you can slice it |\n",
    "| ``xs()``                  | ``df1.xs('index', level='column_index', drop_level=False)``        | Select multi-index dataframe          |\n",
    "| ``query()``               | ``df1a.query('column_index_1==\"index1\" \\| column_index_2==\"indexA\"')`` | Query columns/column indexs using boolean expression |\n",
    "| ``sort_values()``         | ``df1.sort_values(by=['index1','index2,'column1','column2'], ascending=[True, True, True, False])`` | Sort dataframe by values and by index |\n",
    "| ``sort_index()``          | ``df1.sort_index()``                                               | Sort dataframe by index               |\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Proper iteration over DataFrame & Series (without loop)\n",
    "# apply() is used to apply a function along an axis of the DataFrame or on values of Series.\n",
    "# applymap() is used to apply a function to a DataFrame elementwise.\n",
    "# map() is used to substitute each value in a Series with another value.\n",
    "\n",
    "\n",
    "#* To apply function over column/row in DataFrame or in Series\n",
    "df1.apply(function, axis=1)     #* axis=0/'index' for applying function over column, axis=1/'columns' for applying function over row\n",
    "s1.apply(function)\n",
    "\n",
    "df1['new_column'] = df1.apply(lambda x:x.sum(), axis=1)\n",
    "df1.loc['new_row_name'] = df1.apply(lambda x:x.sum(), axis=0)   # Creating new row using loc\n",
    "\n",
    "#* use parameter: (result_type='broadcast','expand','reduce') to change result format.\n",
    "#* Can either define function separately or use lambda. \n",
    "\n",
    "\n",
    "\n",
    "#* To apply function element-wise across the whole DataFrame \n",
    "df1.applymap(function, na_action=None)  #* na_action='ignore' -> don't apply function to NA values\n",
    "df1.applymap(lambda x: len(str(x)))\n",
    "\n",
    "\n",
    "\n",
    "#* To substitute each value in a Series with another value\n",
    "#* map() accepts a dict, a Series, or a function\n",
    "s1.map({'Cat':'Kitten', 'Dog':\"Puppy\"})     #* Values not found in dict are converted to NaN, unless dict has set defaultdict\n",
    "s1.map(lambda x:x - 100, na_action='ignore')      #* na_action='ignore' -> don't apply function to NA values\n",
    "\n",
    "\n",
    "\n",
    "#* Vlookup in dataframe (using map)\n",
    "df1['new_column'] = df1['lookup_values'].map(df2.set_index('lookup_array')['return_array'])     #* Vlookup in df. df2 index must be the lookup_array\n",
    "\n",
    "#* Vlookup in dataframe (using left join)\n",
    "df1.join(df2.set_index('common_column')['return_array'], on='common_column')\n",
    "\n",
    "#* Vlookup in dataframe (using left merge)\n",
    "df1.merge(df2[['common_column', 'return_array']], on='common_column', how='left')\n",
    "df = pd.merge(df1, df2[['common_column', 'return_array']], on='common_column', how='left')\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Data Cleaning\n",
    "\n",
    "\n",
    "<ol>\n",
    "    <li>Problems in data preparation</li>\n",
    "    <ul>\n",
    "        <li>Redundant data: the same data point being stored as separate data points multiple times</li>\n",
    "        <li>Inconsistent data: data points are stored in inappropriate formats</li>\n",
    "        <li>Inaccurate data: wrong values collected leading to wrong conclusions or insights during analysis.</li>\n",
    "        <li>How to process data</li>\n",
    "            <ol type='a'>\n",
    "                <li>Examine the data as a whole.</li>\n",
    "                <li>Understand the contextual meaning for columns and rows. (missing header)</li>\n",
    "                <li>Note the total number of records (rows). (duplicated rows)</li>\n",
    "                <li>The format and unit of data.</li>\n",
    "                <li>Look out for inconsistent formatting. (inconsistent unit)</li>\n",
    "                <li>Handling missing data. (missing & wrong values)</li>\n",
    "            </ol>\n",
    "    </ul>\n",
    "<br>\n",
    "    <li>Look at the dataset. See if it reads correctly and get an idea of the data.</li>\n",
    "        <ul>\n",
    "            <li>df.head( )</li>\n",
    "            <ul><li>df.shape</li>\n",
    "                <li>df.index</li>\n",
    "                <li>df.columns</li>\n",
    "                <li>df.dtypes</li>\n",
    "                <li>df.count( )</li>\n",
    "                    <ul>\n",
    "                    # Count non-NA cells\n",
    "                    </ul>\n",
    "                <li>df.nunique( )</li>\n",
    "                    <ul>\n",
    "                    # Return the number of unique values\n",
    "                    </ul>\n",
    "                <li>df['column1'].unique( )</li>\n",
    "                    <ul>\n",
    "                    # Return all the unique values in the selected column\n",
    "                    </ul>\n",
    "                <li>df.value_counts( )</li>\n",
    "                    <ul>\n",
    "                    # Return all the unique values and their counts\n",
    "                    </ul>\n",
    "        </ul>\n",
    "<br>\n",
    "    <li>Check for and remove duplicates.</li>\n",
    "        <ul>\n",
    "            <li>df.duplicated( )</li>\n",
    "                <ul>\n",
    "                # Return boolean Series denoting duplicate rows (entire row) \n",
    "                </ul>\n",
    "            <li>df.duplicated( ).sum( )</li>\n",
    "            <li>df.drop_duplicates(subset=['col1','col2'], keep='first', inplace=False)</li>\n",
    "                <ul type='none'>\n",
    "                <li> # Will drop the whole row.</li>\n",
    "                <li> # subset: columns to find duplicates, default use all columns.</li>\n",
    "                <li> # keep='first': drop duplicates except first occurrence (default). keep='last': drop duplicates except last occurrence. keep=False: drop all duplicates.</li>\n",
    "                </ul>\n",
    "        </ul>\n",
    "<br>\n",
    "    <li>Got missing data?</li>\n",
    "        <ul>\n",
    "            <li>df.isnull( ).sum( )</li>\n",
    "            <li>% of value missing = df.isnull( ).sum( ).sum( )/np.product(df.shape)</li>\n",
    "            <ul><li>df.info( )</li></ul>\n",
    "        </ul>\n",
    "<br>\n",
    "    <li>Why is the data missing?</li>\n",
    "        <ul>\n",
    "            <li>The value is missing becuase it doesn't exist</li>\n",
    "            <ul>\n",
    "                <li>Then it doesn't make sense to try and guess what it might be. Keep these values as NaN/NA</li>\n",
    "            </ul>\n",
    "            <li>The value is missing because it wasn't recorded</li>\n",
    "            <ul>\n",
    "                <li>Then can try guess what it might be based on the other values in that column and row; aka imputation. </li>\n",
    "            </ul>\n",
    "        </ul>\n",
    "<br>\n",
    "    <li>Dealing with missing data:</li>\n",
    "        <ul>\n",
    "            <li>Drop all rows/columns with missing values (generally not recommended for important projects)</li>\n",
    "            <ul>\n",
    "                <li>df.dropna(axis='index')</li>\n",
    "                <ul>\n",
    "                    # Drop all rows with at least one null value\n",
    "                    <br># axis= 'index', 'columns'\n",
    "                </ul>\n",
    "                <li>df.dropna(how='all')</li>\n",
    "                <ul>\n",
    "                    # Drop rows whose columns' values are all null\n",
    "                </ul>\n",
    "                <li>df.dropna(subset=['column1','column3'])</li>\n",
    "                <ul>\n",
    "                    # Drop rows with null values in columns 'column1' OR 'column3' \n",
    "                </ul>\n",
    "                <li>df.dropna(subset=['column1','column3'], how='all')</li>\n",
    "                <ul>\n",
    "                    # Drop rows with null values in columns 'column1' AND 'column3\n",
    "                </ul>\n",
    "                <li>df.dropna(thresh=len(df.columns)-1)</li>\n",
    "                <ul>\n",
    "                    # Drop rows with more than 1 missing value\n",
    "                    <br># thresh=2 -> Keep rows with at least 2 non-N/A values\n",
    "                </ul>\n",
    "            </ul>\n",
    "            <li>Filling in missing values automatically</li>\n",
    "            <ul>\n",
    "                <li>df.fillna(value={\"emptyColumn1\": 'AAA', \"emptyColumn2\": 'BBB'})</li>\n",
    "                <ul>\n",
    "                    # Assigning explicit values to NA column by column.\n",
    "                </ul>\n",
    "                <li>df.fillna(value={\"emptyIndexA\": 0, \"emptyIndexB\": 1})</li>\n",
    "                <ul>\n",
    "                    # Assigning explicit values to NaN row by row.\n",
    "                </ul>\n",
    "                <li>df.fillna(method='bfill', axis='index').fillna(0)</li>\n",
    "                <ul>\n",
    "                    # Replace all NA with the value that comes directly after it in the same column. Then replace all the remaining na's with 0.\n",
    "                </ul>\n",
    "                <li>df.fillna(method='ffill', axis='index').fillna(0)</li>\n",
    "                <ul>\n",
    "                    # Replace all NA with the value that comes directly before it in the same column. Then replace all the remaining na's with 0.\n",
    "                </ul>\n",
    "                <li>What value to fill?</li>\n",
    "                <ul>\n",
    "                    <li>Values of highest occurrences</li>\n",
    "                    <li>Appropriate statistical value</li>\n",
    "                    <ul>\n",
    "                        <li>df.mean(axis=0, skipna=False)</li>\n",
    "                        <li>df.median(axis=0, skipna=False)</li>\n",
    "                        <li>df.mode(axis=0)</li>\n",
    "                    </ul>\n",
    "                    <li>Appropriate predictive model or algorithm</li>\n",
    "                </ul>\n",
    "            </ul>\n",
    "        </ul>\n",
    "<br>\n",
    "    <li>Transforming numeric variables:</li>\n",
    "        <ul>\n",
    "        <li>Scaling (Changing the range of your data to fit within a specific scale)</li>\n",
    "            <ul>\n",
    "                <li>Methods that are based on measures of how far apart data points are, i.e. support vector machines (SVM) or k-nearest neighbors (KNN), treat changes in any numeric feature with the same importance. (But is 1kg change in weight equivalent to only 1m change in height?) Therefore, need to scale the variables in order to compare the different variables on equal footing. </li>\n",
    "                <li>Min_max scaling</li>\n",
    "                <ul>\n",
    "                    <li>Note that the shape of the data doesn't change, but the data range is limited between 0 to 1.</li>\n",
    "                </ul>\n",
    "            </ul>\n",
    "        <li>Normalization (Changing the data to fit normal distribution)</li>\n",
    "            <ul>\n",
    "                <li>Need to normalize data in order to use machine learning or statistics techniques assumes your data is normally distributed, i.e. near discriminant analysis (LDA) and Gaussian naive Bayes.</li>\n",
    "                <li>Box-Cox Transformation</li>\n",
    "                <ul>\n",
    "                    <li>Note that the shape of the data has changed to look more like a normal distribution.</li>\n",
    "                </ul>\n",
    "            </ul>\n",
    "        </ul>\n",
    "<br>\n",
    "    <li>Parsing dates:</li>\n",
    "        <ol type=\"a\">\n",
    "        <li>Check the data type of date column</li>\n",
    "        <ul>\n",
    "            <li>df['date'].head( )</li>\n",
    "            <li>df['date'].dtype( )</li>\n",
    "            <li>df['date'].str.len( ).value_counts( )</li>\n",
    "        </ul>\n",
    "        <li>Convert date columns to datetime64</li>\n",
    "        <ul>\n",
    "            <li>Python strftime cheatsheet: strftime.org</li>\n",
    "            <li>pd.to_datetime(df['date'], format='%m/%d/%y')</li>\n",
    "            <li>pd.to_datetime(df['date'], infer_datetime_format=True)</li>\n",
    "            <ul>\n",
    "                # when there are multiple date formats in a single column (may not always work)\n",
    "            </ul>\n",
    "        </ul>\n",
    "        <li>Extracting info from parsed dates</li>\n",
    "        <ul>\n",
    "            <li>df['datetime'].dt.date</li>\n",
    "            <ul>\n",
    "                # Pandas .dt methods: .day, .month, .year, .week, .dayofweek\n",
    "            </ul>\n",
    "            <li>df['datetime'].dt.strftime('%m/%d/%Y')</li>\n",
    "            <ul>\n",
    "                # changing the datetime format\n",
    "            </ul>\n",
    "        </ul>\n",
    "        <li>Double-check that days & months are not mixed up</li>\n",
    "        <ul>\n",
    "            <li>Plot a histogram of the days of the month. Value should be between 1 and 31, and relatively evenly distributed, with dip on 31.</li>\n",
    "            <li>sns.distplot(df['date'].dropna(), kde=False, bins=31)</li>\n",
    "        </ul>\n",
    "        </ol>\n",
    "</ol>\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# library for min_max scaling\n",
    "from mlxtend.preprocessing import minmax_scaling\n",
    "\n",
    "# general modules to import\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "df1 = pd.read_csv()\n",
    "original_data = pd.DataFrame(df1.column1)\n",
    "\n",
    "# mix-max scale the data between 0 and 1\n",
    "scaled_data = minmax_scaling(original_data, columns=['column1'])\n",
    "\n",
    "# plot the original & scaled data together to compare\n",
    "fig, ax = plt.subplots(1,2)\n",
    "sns.distplot(original_data, ax=ax[0])\n",
    "ax[0].set_title(\"Original Data\")\n",
    "sns.distplot(scaled_data, ax=ax[1])\n",
    "ax[1].set_title(\"Scaled data\")\n",
    "\n",
    "# after scaling, all values lie between 0 and 1\n",
    "print(original_data.head())\n",
    "print(original_data.min())\n",
    "print(original_data.max())\n",
    "print(scaled_data.head())\n",
    "print(scaled_data.min())\n",
    "print(scaled_data.max())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for Box-Cox Transformation\n",
    "from scipy import stats\n",
    "\n",
    "# general modules to import\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# normalize the exponential data with boxcox\n",
    "normalized_data = stats.boxcox(original_data)\n",
    "\n",
    "# Plot both together to compare. \n",
    "fig, ax=plt.subplots(1,2)\n",
    "sns.distplot(original_data, ax=ax[0])\n",
    "ax[0].set_title(\"Original Data\")\n",
    "sns.distplot(normalized_data[0], ax=ax[1])\n",
    "ax[1].set_title(\"Normalized data\")\n",
    "\n",
    "# after normalization\n",
    "print(original_data.head())\n",
    "print(original_data.min())\n",
    "print(original_data.max())\n",
    "print(scaled_data.head())\n",
    "print(scaled_data.min())\n",
    "print(scaled_data.max())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Reading files with encoding problems\n",
    "import chardet\n",
    "\n",
    "#* look at the first 10,000 bytes to guess the character encoding\n",
    "with open(\"EncodedData.csv\", 'rb') as rawdata:\n",
    "    result = chardet.detect(rawdata.read(100000))   #* may need to read more than 10,000 byte\n",
    "\n",
    "#* check what the character encoding might be\n",
    "print(result)\n",
    "\n",
    "#* read in the file with the encoding detected by chardet\n",
    "df1 = pd.read_csv(\"EncodedData.csv\", encoding='Windows-1252')\n",
    "\n",
    "#* save our file (will be saved as UTF-8 by default!)\n",
    "df1.to_csv(\"df1-utf8.csv\")\n",
    "\n",
    "# If you have a UTF-8 file that has a couple of weird-looking characters, try out the ftfy module.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## cleaning up inconsistent text entries\n",
    "import fuzzywuzzy\n",
    "from fuzzywuzzy import process\n",
    "import chardet\n",
    "\n",
    "#* Quick look at data\n",
    "df1.head()\n",
    "\n",
    "#* Get all the unique values to identify inconsistencies\n",
    "df1['column1'].unique().sort()\n",
    "\n",
    "\n",
    "#* Convert all to lower case\n",
    "df1['column1'] = df1['column1'].str.lower()\n",
    "\n",
    "#* Remove trailing white spaces\n",
    "df1['column1'] = df1['column1'].str.strip()\n",
    "\n",
    "\n",
    "#* Use fuzzy matching to identify variants of the same text value\n",
    "matches = fuzzywuzzy.process.extract(\"base_text_value\", df1['column1'].unique(), limit=10, scorer=fuzzywuzzy.fuzz.token_sort_ratio)     #* get the top 10 closest matches to \"base_text_value\"\n",
    "\n",
    "#* define function that will replace all text variants with 1 common \"base_text_value\"\n",
    "def replace_variants(df, column, base_text_value_to_match, min_token_sort_ratio):\n",
    "    # get uniques\n",
    "    column_unique = df[column].unique() \n",
    "    \n",
    "    # get the top 10 closest matches to our input \"base_text_value\"\n",
    "    matches = fuzzywuzzy.process.extract(base_text_value_to_match, column_unique, limit=10, scorer=fuzzywuzzy.fuzz.token_sort_ratio)\n",
    "    \n",
    "    # only get matches with a ratio > min_token_sort_ratio\n",
    "    close_matches = [match[0] for match in matches if match[1] >= min_token_sort_ratio]      # matches = list of tuple. matches[0] = unique value. matches[1] = token_sort_ratio.\n",
    "    \n",
    "    # get the rows of all the close matches in our dataframe\n",
    "    rows_with_matches = df[column].isin(close_matches)\n",
    "    \n",
    "    # replace all rows with close matches with the input matches\n",
    "    df.loc[rows_with_matches, column] = base_text_value_to_match\n",
    "    \n",
    "    "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Data Visualization Components\n",
    "1. Data (Is this the right data to plot with?)\n",
    "2. Type of chart\n",
    "3. Label\n",
    "4. Axis\n",
    "5. Dimension (2D or 3D graph?)\n",
    "\n",
    "#### Purpose of visualization\n",
    "<ol>\n",
    "    <li>Form the big idea or design a central message:</li>\n",
    "    <ul>\n",
    "        <li>What story are you trying to tell?</li>\n",
    "        <li>What question are you trying to answer?</li>\n",
    "        <li>What message you are trying to convey?</li>\n",
    "    </ul>\n",
    "    <li>Gather related data to support the big idea or message:</li>\n",
    "    <ul>\n",
    "        <li>Context should be illustrated for effective visualization.</li>\n",
    "        <li>What data are we examining? What is the background of the data?</li>\n",
    "        <li>What is the timeframe of this data?</li>\n",
    "        <li>Who are the target audience?</li>\n",
    "        <li>Nature of visualization - for explaining or exploring?</li>\n",
    "        <li>Any impacts you hope to achieve with the visualization?</li>\n",
    "        <li>Any any change you hope to see?</li>\n",
    "    </ul>\n",
    "    <li>Choose appropriate type of visualization that strengthens your key point:</li>\n",
    "    <ul>\n",
    "        <li>Articulate and emphasise the main point with bolded big title or colour</li>\n",
    "    </ul>\n",
    "</ol>\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Data Visualization\n",
    "import pandas as pd\n",
    "# pd.plotting.register_matplotlib_converters()\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.dates as mdates\n",
    "from matplotlib.ticker import MultipleLocator\n",
    "# %matplotlib inline\n",
    "import seaborn as sns\n",
    "from scipy import stats\n",
    "import statsmodels.api as sm\n",
    "\n",
    "\n",
    "\n",
    "# Tabulation table (for univariate/bivariate categorical variables)\n",
    "#* Manually\n",
    "df1 = df1.groupby(['col1', 'col2', 'col3']).size().unstack()\n",
    "df1 = df1.groupby('col1').col2.agg([lambda x:np.mean(x=='Yes'), np.std, np.size])\n",
    "\n",
    "#* Using crosstab\n",
    "df1 = pd.crosstab(df1['col1'], df1['col2'], margins=False, normalize=False)  # normalize = 'all', 'index', 'columns'\n",
    "\n",
    "#* Normalise table to proportion\n",
    "df1.apply(lambda x: x/x.sum(), axis=1)  # Normalise within row\n",
    "# 3 ways to normalise a table, make the rows sum to 1, columns sum to 1, or whole table sum to 1\n",
    "\n",
    "\n",
    "\n",
    "#* Pandas plots\n",
    "df1['col1'].plot(kind='bar', x='x_axis_name', y='y_axis_name', color='what color', title='title', legend=False)\n",
    "# kind = 'line', 'bar', 'barh', 'hist', 'box', 'kde'/'density', 'area', 'pie', 'scatter', 'hexbin'\n",
    "\n",
    "df1.plot.scatter(x='x_axis_name', y='y_axis_name', s=df1['3rd variable']*1000)\n",
    "# Plot 3D plot, 3rd variable showing up as size of dots\n",
    "\n",
    "\n",
    "\n",
    "#* Pyplot plots\n",
    "plt.plot(df1['col1'], df1['col2'], color='black', marker='*', linestyle='None')\n",
    "plt.show()\n",
    "# Plot 2D plot\n",
    "\n",
    "\n",
    "\n",
    "# Charts showing comparison between categorical variables\n",
    "\n",
    "#? Bar Chart    (comparing quantities corresponding to different groups/categories)\n",
    "#* Plot Bar Chart\n",
    "sns.barplot(x=df1['column1'], y=df1['column2'], estimator='mean')     #* use df1.index if using index as axis\n",
    "sns.barplot(data= {'A':['a','b','c'], 'B':[10,20,30]}, x='A', y='B')    #* group data correctly using dictionary\n",
    "#* switch x-axis and y-axis around to rotate figure\n",
    "#* plt.xlabel(\"\") to remove horizontal label\n",
    "#* ax.bar_label(ax.containers[0]) to show bar values\n",
    "#* by default, barplot displays mean (estimator='mean'). other estimators: np.sum, np.median, np.min, np.max, len\n",
    "\n",
    "#* Plot Point Plot (categorical data / plot measure of central tendency)\n",
    "sns.pointplot(x=df1.index, y=df1['column1'], label='label in legend')      #* use label='' to set the values in the legend for all types of graph\n",
    "sns.pointplot(y=[1,1], x=[-10,10], color=\"r\")   # loop to plot vertical lines plot\n",
    "# can plot confidence interval too\n",
    "\n",
    "#* Plot Strip Plot (categorical scatterplot)\n",
    "sns.stripplot(data=df1, x='col1', y='col2', hue='col3', jitter=True)\n",
    "\n",
    "#* Plot Swarm Plot (categorical scatterplot)\n",
    "sns.swarmplot(data=df1, x='col1', y='col2', hue='col3')\n",
    "\n",
    "#* Plot Cat Chart\n",
    "sns.catplot(x=df1['column1'], y=df1['column2'])\n",
    "\n",
    "#? Bar Chart    (plot value counts)\n",
    "#* Plot Side-By-Side Bar Chart \n",
    "sns.countplot(x=df1['column1'], hue=df1['column2'], order=['A','B','C'])                                    # plot count\n",
    "pd.crosstab(df1['col1'], df1['col2'], margins=True, normalize='all').plot(kind='bar', figsize=(12,10))      # plot percentage\n",
    "\n",
    "#* Plot Stacked Bar Chart \n",
    "df1.groupby(['column1', 'column2']).size().unstack().plot(kind='bar', stacked=True)  # easiest way to plot stacked bar\n",
    "\n",
    "#? Boxplot      (analyse 1 categorical variable, or between continuous & categorical variables)\n",
    "sns.boxplot(data=df1['column1'])\n",
    "sns.boxplot(data=df1, x=\"flower_length\", y='flower_type')   #* side-by-side comparison of 1 quantitative & 1 categorical variable\n",
    "# use 'hue' to add 3rd categorical variable\n",
    "# use 'order=['a','b']' to control the how categorical variables are positioned\n",
    "\n",
    "#? Violinplot   (boxplot + distribution shape)\n",
    "sns.violinplot(data=df1, x=\"flower_length\", y='flower_type')\n",
    "\n",
    "#? Heatmap      (finding color-coded patterns in tables of numbers)\n",
    "#* Plot Heatmap\n",
    "sns.heatmap(data=df1, annot=True)       #* annot=True -> show the values for each cell \n",
    "# use annot=df1.rank() to rank and annotate data \n",
    "\n",
    "#* Plot Correlation Matrix\n",
    "mask = np.triu(np.ones_like(df1.corr(), dtype=bool))        # Set mask to plot half triangle\n",
    "cmap = sns.diverging_palette(230, 20, as_cmap=True)         # Set colour map\n",
    "sns.heatmap(df1.corr(), linewidth=0.5, cmap=cmap, mask=mask)\n",
    "\n",
    "\n",
    "\n",
    "# Charts showing trends (to show a pattern of change overtime):\n",
    "#? Line Chart\n",
    "#* Plot Line Plot (time series)\n",
    "sns.lineplot(x=df1[\"date\"], y=df1['EPS'], linestyle='dashed', marker='o', label='label in legend')\n",
    "\n",
    "\n",
    "\n",
    "# Charts showing relationship between continuous variables:\n",
    "#? Scatter Plot     (showing relationship between variables)\n",
    "#* Plot Scatter Plot for 2 continuous variables\n",
    "sns.scatterplot(x=df1['column1'], y=df1['column2'])\n",
    "\n",
    "#* Plot Scatter Plot for 2 continuous variables with Regression Line\n",
    "sns.regplot(x=df1['column1'], y=df1['column2'], ci=95, order=1, logistic=False, scatter_kws={\"color\": \"blue\", 'alpha':0.3}, line_kws={\"color\": \"red\"})\n",
    "\n",
    "#* Plot colour-coded Scatter Plot for 2 continuous variables & 1 categorical variable\n",
    "sns.scatterplot(x=df1['column1'], y=df1['column2'], hue=df1['column3'])     #* data points will be colour coded based on categorical 'column3'\n",
    "\n",
    "#* Plot colour-coded Scatter Plot with multiple Regression Lines for the different segments/colour-coded groups\n",
    "sns.lmplot(x=\"column1\", y=\"column2\", hue=\"column3\", data=df1)\n",
    "\n",
    "#* Plot Scatter Plot for 1 continuous variable & 1 categorical variable\n",
    "sns.swarmplot(x=df1['column1'], y=df1['column3'])\n",
    "\n",
    "# to mitigate overplotting, use 'alpha' to make the points semi-transparent\n",
    "# or plot the density of the points using jointplot(kind='kde'), where plot margins show the \n",
    "# X and Y densities separately, while center plot shows their density jointly\n",
    "\n",
    "\n",
    "\n",
    "# Charts showing correlation:\n",
    "#? Matric plot\n",
    "sns.pairplot(df1)\n",
    "\n",
    "\n",
    "\n",
    "# Charts showing distribution:\n",
    "#? Histogram        (showing the distribution of a single numerical variable)\n",
    "#* Plot Histogram\n",
    "sns.histplot(data=df1['column1'], binwidth=10)\n",
    "\n",
    "#? Kernel Density Estimate (KDE)    (showing an estimated, smooth distribution of a single numerical variable (or two numerical variables))\n",
    "#* Plot KDE Plot\n",
    "sns.kdeplot(data=df1['column1'], shade=True)\n",
    "\n",
    "#* Plot 2D KDE Plot\n",
    "sns.jointplot(x=df1['column1'], y=df1['column2'], kind=\"kde\")\n",
    "# 2D KDE plot in the center -> how likely it is to see the different combinations of column1 and column2; where darker parts of the figure are more likely\n",
    "# curve at the top of the figure is a KDE plot for the data on the x-axis\n",
    "# curve on the right of the figure is a KDE plot for the data on the y-axis\n",
    "\n",
    "\n",
    "\n",
    "# Statistical charts:\n",
    "#? Empirical cumulative distribution function\n",
    "sns.ecdfplot(data=df1, x=df1['column1'])\n",
    "\n",
    "#? Probability density function\n",
    "x = np.linspace(np.min(df1['column1']), np.max(df1['column1']), 100)\n",
    "y = stats.norm.pdf(x, np.mean(df1['column1']), np.std(df1['column1']))\n",
    "ax.plot(x, y, 'r', label='pdf')\n",
    "\n",
    "#? qqplot\n",
    "sm.qqplot(df1['column1'], fit=True, line='45')\n",
    "\n",
    "\n",
    "\n",
    "# Plot multiple charts together\n",
    "#? 2 graphs together\n",
    "#* Using matplotlib\n",
    "fig, ax = plt.subplots()    # fig = plt.figure(), ax = fig.subplot()\n",
    "ax.scatter(df1['colA'], df1['colB'])\n",
    "ax.scatter(df2['col1'], df2['col2'])\n",
    "plt.show()\n",
    "\n",
    "#* Using seaborn\n",
    "sns.histplot(df1['colA'], ax=ax)\n",
    "sns.histplot(df2['col1'], ax=ax)\n",
    "\n",
    "#* Using pandas plot\n",
    "ax = df1.plot()\n",
    "df2.plot(ax=ax)\n",
    "\n",
    "#? Subplots\n",
    "#* 2 vertically stacked subplots\n",
    "fig, ax = plt.subplots(2, sharex=True, figsize=(10,8), dpi=300)\n",
    "ax[0].plot(df1['colA'], df1['col1'])\n",
    "ax[1].plot(df1['colA'], df1['col2'])\n",
    "plt.show()\n",
    "\n",
    "#* 2 side-by-side subplots\n",
    "fig, ax = plt.subplots(1,2)\n",
    "ax[0].plot(df1['colA'], df1['col1'])\n",
    "ax[1].plot(df1['colA'], df1['col2'])\n",
    "plt.show()\n",
    "\n",
    "#* Using seaborn\n",
    "fig, ax = plt.subplots(1,2, sharey=True, sharex=True)\n",
    "sns.barplot(data=df1, x='col1', y='col2', hue='col3', ax=ax[0])\n",
    "sns.barplot(data=df1, x='colA', y='col2', hue='col3', ax=ax[1])\n",
    "\n",
    "#* Using FacetGrid (plot multiple graphs to explore all conditional relationship)\n",
    "g = sns.FacetGrid(df1, row='column1', col='column2')    # set up the figures and axes\n",
    "g.map(sns.scatterplot, 'column3', 'column4')    # provide plotting function and variables\n",
    "g.map(sns.histplot, 'column3').add_legend()\n",
    "\n",
    "\n",
    "\n",
    "# Useful commands:\n",
    "#* Set width and height of the figure\n",
    "plt.figure(figsize=(12,10))\n",
    "\n",
    "#* Add straight line\n",
    "plt.axhline(y=0.5, color='r', linestyle='-')    # horizontal line\n",
    "plt.axvline(x=0.5, color='r', linestyle='-')    # vertical line\n",
    "\n",
    "#* Highlight X range (by adding rectangle across axis)\n",
    "import datetime\n",
    "plt.axvspan(xmin=datetime.datetime(2020,3,12), xmax=datetime.datetime(2020,6,1), color=\"red\", alpha=0.3, label=\"recession\")\n",
    "\n",
    "#* Add title\n",
    "#? First method\n",
    "plt.title(\"Title of the Chart\")\n",
    "#? Second method\n",
    "ax.set_title(\"Title of the Chart\")\n",
    "\n",
    "#* Add label for horizontal axis\n",
    "#? First method\n",
    "plt.xlabel(\"x-axis title\")\n",
    "#? Second method\n",
    "ax.set_xlabel(\"x-axis title\")\n",
    "\n",
    "#* Add label for vertical axis\n",
    "#? First method\n",
    "plt.ylabel(\"y-axis title\")\n",
    "#? Second method\n",
    "ax.set_ylabel(\"y-axis title\")\n",
    "\n",
    "#* Set Y axis range\n",
    "plt.ylim(0, 60000)\n",
    "\n",
    "#* Set X axis range\n",
    "#? First method\n",
    "plt.xlim(datetime.datetime(2010,1), datetime.datetime(2019,12))\n",
    "#? Second method\n",
    "plt.xticks(ticks=np.arange(2010, 2020, 5), labels=[2010, 2015, 2020])\n",
    "\n",
    "#* Format major tick labels (Formatters control the formatting of tick labels)\n",
    "#? Y-axis\n",
    "ax.yaxis.set_major_formatter('{x:1.2f}')\n",
    "#? X-axis\n",
    "ax.xaxis.set_major_formatter(mdates.ConciseDateFormatter(ax.xaxis.get_major_locator()))     # Concise Date Formatter\n",
    "ax.xaxis.set_major_formatter(mdates.DateFormatter('%Y-%m-%d'))                              # Manual Date Formatter\n",
    "ax.xaxis.set_major_formatter(mdates.ConciseDateFormatter(mdates.AutoDateLocator(minticks=15, maxticks=20)))\n",
    "ax.xaxis.set_major_formatter(mdates.AutoDateFormatter(mdates.AutoDateLocator()))\n",
    "# Must explicitly assign time_column as x-value to use DateFormatter!\n",
    "\n",
    "#* Format minor tick labels (Formatters control the formatting of tick labels)\n",
    "#? Y-axis\n",
    "ax.yaxis.set_minor_formatter('{x} km')\n",
    "#? X-axis\n",
    "ax.xaxis.set_minor_formatter('{x:1.1f} cm')\n",
    "\n",
    "#* Format major ticks (Locators determine where the ticks are)\n",
    "#? Y-axis\n",
    "ax.yaxis.set_major_locator()\n",
    "#? X-axis\n",
    "ax.xaxis.set_major_locator(MultipleLocator(20))\n",
    "ax.xaxis.set_major_locator(mdates.AutoDateLocator())\n",
    "ax.xaxis.set_major_locator(mdates.MonthLocator(bymonth=(1, 7)))     #  Major ticks every half year (Jan/Jul)\n",
    "# possible locators: mdates.DayLocator(), mdates.HourLocator(range(0, 25, 6))\n",
    "\n",
    "#* Format minor ticks (Locators determine where the ticks are)\n",
    "#? Y-axis\n",
    "ax.yaxis.set_minor_locator()\n",
    "#? X-axis\n",
    "ax.xaxis.set_minor_locator(mdates.MonthLocator())                   # Minor ticks every month\n",
    "\n",
    "#* Set and format both ticks and tick labels manually\n",
    "#? X-axis\n",
    "plt.xticks(ticks=np.arange(0, 21, 10), labels=['zero', 'ten', 'twenty'], minor=False, rotation=45)\n",
    "plt.xticks(np.arange(0, df1['timeseriesdatescol'].size + 1, 12), np.arange(1949, 1962, 1))         # Set yearly x-axis for time series\n",
    "\n",
    "#* Set secondary axis\n",
    "ax2 = ax.twinx()\n",
    "ax2.set_ylim(4, 20)\n",
    "ax2.set_ylabel('Y-title', color='b')\n",
    "\n",
    "#* Set log axes\n",
    "#? First method \n",
    "fig, ax = plt.subplots()\n",
    "ax.set_yscale('log')\n",
    "ax.set_xscale('log')\n",
    "#? Second method\n",
    "plt.yscale('log',base=2)\n",
    "#? Third method\n",
    "df1.plot(logy=True, logx=True, loglog=True)\n",
    "\n",
    "#* Force legend to appear\n",
    "#? First method \n",
    "plt.legend()\n",
    "#? Second method\n",
    "ax.legend()\n",
    "\n",
    "#* Shift & modify legend\n",
    "plt.legend(loc=(1.05,0.5), frameon=True)\n",
    "\n",
    "#* Rename legend\n",
    "#? First method\n",
    "fig, ax = plt.subplots()\n",
    "ax.legend(df1['colA'].unique())\n",
    "#? Second method\n",
    "plt.legend(['one','two', 'three'])\n",
    "\n",
    "#* Change graph colours\n",
    "sns.color_palette(\"dark\")\n",
    "\n",
    "#* Change the style of the figure to the \"dark\" theme\n",
    "sns.set_style(\"dark\")\n",
    "# \"darkgrid\", \"whitegrid\", \"dark\", \"white\", \"ticks\"\n",
    "\n",
    "#* Set plot grid\n",
    "ax.grid(True)\n",
    "\n",
    "#* Set plot spine\n",
    "ax.spines.right.set_visible(False)                              # Hide right spine\n",
    "ax.spines.top.set_visible(False)                                # Hide top spine\n",
    "ax.spines.left.set_bounds(df1.ycol.min(), df1.ycol.max())       # Only draw spines for data range\n",
    "ax.spines.bottom.set_bounds(df1.xcol.min(), df1.xcol.max())     # Only draw spines for data range\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Itertools\n",
    "import itertools\n",
    "\n",
    "\n",
    "#* List of even integers using itertools.count()\n",
    "iterator = (itertools.count(start = 0, step = 2))\n",
    "x = [next(iterator) for i in range(5)]\n",
    "\n",
    "#* Emulating enumerate() using itertools.count()\n",
    "x = list(zip(itertools.count(), [100,200,300]))\n",
    "y = list(itertools.zip_longest(range(10), [100,200,300]))\n",
    "\n",
    "\n",
    "\n",
    "#* Sequence generation\n",
    "counter = itertools.cycle(['on', 'off'])\n",
    "x = [next(counter) for i in range(5)]\n",
    "\n",
    "\n",
    "\n",
    "#* Supplying constant to map() and zip()\n",
    "counter = itertools.repeat(1,10)\n",
    "x = list(counter)\n",
    "\n",
    "squares = map(pow, range(10), itertools.repeat(2))\n",
    "y = (list(squares))\n",
    "\n",
    "\n",
    "\n",
    "#* starmap(): map function specifically for tuples\n",
    "squares = itertools.starmap(pow, [(0,2), (1,2), (2,2)])\n",
    "y = (list(squares))\n",
    "\n",
    "\n",
    "#* Cartesian products\n",
    "x = list(itertools.product([1,2,3,4], repeat=2))\n",
    "y = list(itertools.product(range(4), range(4)))\n",
    "\n",
    "\n",
    "#* Combinations and Permutations\n",
    "x = itertools.combinations(['a', 'b', 'c', 'd'], 2)\n",
    "for i in x:\n",
    "    print(i)\n",
    "\n",
    "y = itertools.combinations_with_replacement([0, 1, 2, 3], 4)\n",
    "for i in y:\n",
    "    print(y)\n",
    "\n",
    "z = itertools.permutations(['a', 'b', 'c', 'd'], 2)\n",
    "for i in z:\n",
    "    print(i)\n",
    "\n",
    "\n",
    "\n",
    "#* Loop through all iterables and returns one iterable\n",
    "x = itertools.chain([1, 2, 3], 'ABC', ['DEF', 'GHI'])\n",
    "print(list(x))\n",
    "\n",
    "#* Takes a singe iterable and flatten all the elements it contains\n",
    "y = itertools.chain.from_iterable(['ABC', 'DEF', 'GHI', 'JKL'])\n",
    "print(list(y))\n",
    "# itertools.chain and itertools.chain.from_iterable can be used to combine list of lists of strings into 1 list\n",
    "\n",
    "\n",
    "\n",
    "#* Slicing by index (without loading data into memory)\n",
    "x = itertools.islice(range(10), 1, 5, 2)\n",
    "\n",
    "with open('test.log', 'r') as f:\n",
    "    header = itertools.islice(f, 3)     # get first 3 lines of the log file\n",
    "    for line in header:\n",
    "        print(line)\n",
    "\n",
    "#* Slicing by boolean list\n",
    "x = itertools.compress(['a', 'b', 'c', 'd'], [False, True, 0, 1])\n",
    "print(list(x))\n",
    "# Similar to filter()\n",
    "\n",
    "\n",
    "\n",
    "#* Return the running accumulated results of a function looping through an iterator\n",
    "x = itertools.accumulate([0, 1, 2, 3, 4, 5, 6])     # default function is sum()\n",
    "for i in x:\n",
    "    print(i)\n",
    "\n",
    "y = itertools.accumulate([0, 1, 2, 3, 4, 5, 6], operator.mul)\n",
    "for i in y:\n",
    "    print(i)\n",
    "# Similar to functools.reduce() -> only returning the final accumulated result\n",
    "\n",
    "\n",
    "\n",
    "#* itertools.groupby()\n",
    "x_raw = [(1, \"a\"), (2, \"a\"), (3, \"b\"), (4, \"b\"), (5, \"a\"), (6, \"a\"), (7, \"c\")]\n",
    "x_sorted = sorted(x_raw, key = lambda x:x[1])\n",
    "for key, group in itertools.groupby(x_sorted, key = lambda x:x[1]):\n",
    "    print(f'{key}: {list(group)}')\n",
    "\n",
    "def check_even(n):\n",
    "    if n%2 == 0:\n",
    "        return 'even'\n",
    "    else:\n",
    "        return 'odd'\n",
    "def group_numbers(iterables):\n",
    "    iterables_sorted = sorted(iterables, key=check_even)\n",
    "    for key, group in itertools.groupby(iterables_sorted, key = check_even):\n",
    "        print(f'{key}: {list(group)}')\n",
    "group_numbers([10,25,45,50])\n",
    "\n",
    "# Group together consecutive items that are of the same occurrence\n",
    "# Always first sort items with the same key to be used for grouping\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## RegEx\n",
    "import re\n",
    "\n",
    "#* Returns a list of strings containing all matches\n",
    "re.findall(r\"pattern\", \"string\")    # r'' -> raw string\n",
    "\n",
    "#* Returns a list of strings where the splits have occurred\n",
    "re.split(r'\\d+', \"string\") \n",
    "\n",
    "#* Returns a string with substitution done\n",
    "re.sub(r\"pattern\", \"replace\", \"string\") \n",
    "re.sub(\"[^A-Za-z0-9]+\", \" \", \"string\")       # remove characters that are not a letters or numbers\n",
    "\n",
    "#* Returns match object(s)\n",
    "re.search(r\"pattern\", \"string\").group()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Misc\n",
    "\n",
    "#* Remove apostrophe from string\n",
    "df1.replace(\"'\", \"\")\n",
    "\n",
    "#* Searching for specific string values\n",
    "df2 = df1[df1['column1'].str.contains('string1')]\n",
    "\n",
    "#* Count all non-zero row in each column\n",
    "df1.astype(bool).sum(axis=0)\n",
    "'''OR'''\n",
    "pd.read_csv('file.csv', keep_default_na=True).shape\n",
    "\n",
    "#* Open/Read all (JSON) files in the folder\n",
    "import glob\n",
    "allfiles = glob.glob(\"C:\\\\Users\\\\YX\\\\Documents\\\\Rate Calculator\\\\File_name\\\\**.json\")\n",
    "for jsonfiles in allfiles:\n",
    "    df = pd.read_json(jsonfiles)\n",
    "\n",
    "#* returns True if any item in an iterable are true, else returns False\n",
    "any()\n",
    "\n",
    "#* map() {not series.map()}\n",
    "x = map(function, ['aaa', 'bbb', 'ccc'])\n",
    "# executes function for each item in an iterable\n",
    "\n",
    "#* filter()\n",
    "x = filter(function, ['aaa', 'bbb', 'ccc'])\n",
    "# filters an iterable through a function and select items that return True \n",
    "\n",
    "#* Pass elements in iterable through a function and filter those that return True \n",
    "filter(function, 'iterable to be filtered')\n",
    "# similar to itertools.compress and (i for i in iterable if function(item))\n",
    "# opposite of itertools.filterfalse\n",
    "\n",
    "#* Sorted()\n",
    "x = sorted('iterable', key=None, reverse=False)     # similar to list.sort(*, key=None, reverse=False) -> this will overwrite original data\n",
    "# e.g. of key: len, lambda x:x%7\n",
    "# Can only sort similar data types at a time\n",
    "\n",
    "#* Split string values in Series (into DataFrame)\n",
    "df1['col1'].str.split(pat=None, expand=False)\n",
    "# expand=False: return list of strings. expand=True: expand split strings into separate columns in DataFrame.\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.10.0 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "5a8e7d507ea98077b019e09bf543f298110182c9fad4ab74c1ed583aa19b9ac6"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
